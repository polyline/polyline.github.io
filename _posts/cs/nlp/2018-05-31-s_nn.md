---
layout: article_post
title:  "[讀]Neural Network"
categories: cs
excerpt_separator: <!--more-->
---

*2018-5-31*

---
## 介紹
---

現在邁入`Speech and Language Processing`這本書的`第八章Neural Networks and Neural Language Models`

這章的內容跟之前在`Coursera`上過的`Machine Learning`課程有很大的重疊，所以可以當作複習來看

這篇文章中，會盡量整理一下重要的觀念、術語、公式

<!--more-->

---
## Index
---

1. Unit
2. The XOR Problem
3. Feed-Forward Neural Networks
4. Training Neural Nets
5. Neural Language Models
6. Summary

---
## Feed-Forward Neural Netwroks
---

在這一章，介紹了最基本的`Feed-Forward Neural Networks`，也就是沒有cycles的Neural Networks

最簡單的情況下，分為`input layer`, `hidden layer`, `output layer`

如果在兩層layers之間，每個unit都互相連接的話，我們稱為`fully-connected`

基本每一次計算包含三個步驟

1. wieght matrix W multiplied by input vector X
2. adding bias vector b
3. applying the activation function (non-linear function)

$$h=\sigma(Wx+b)$$

假設我們現在只有一層hidden layer，那我們架構會是

$$dim^{input layer} \rightarrow dim^{hidden layer} \rightarrow dim^{output layer}$$

通常我們最後的`output`會有兩大類:

如果是`binary classification`的話，我們只會有一個output node

而如果是`multinominal classification`，最後應該會有很多個output node，並且`sum=1`

要做到這件事情，我們可以使用`softmax`

$$softmax(z_{i}) = \frac{e^{z_{i}}}{\sum_{j=1}^{k}e^{z_{j}}}$$

這樣一來，最後的output就會是經過`normalize`後的機率

最後整理我們整個neural network的運算過程

$$h=\sigma(Wx+b)$$

$$z=Uh$$

$$y=softmax(z)$$

`Logistic Regression`跟`Neural Network`是兩個不同的東西

`Logistic Regression`是指從`已知的features`中，找出最可能的`class`，也就是

$$ \underset{c}{argmax} P(c|x)$$

我們可以把`Neural Network`的最後一個步驟看作`Logistic Regression`使用`hidden layer`發展出來的`features`

或者把`Logistic Regression`看作沒有`hidden layer`的`Neural Network`，雖然這樣說不太準確

---
## Traing Neural Nets
---

這一小節主要在講`Loss Function`跟`Stochastic Gradient Descent`

比較值得提的有以下幾點

1. 有兩種常見的Loss function: mean squared error, cross entropy loss
2. Stochastic Gradient Descent和mini-batch Gradient Descent
3. learning rate的取值

對於gradient descent的理論跟公式大家應該都很熟悉，通常在結果是機率的情況，我們的loss function會採用`cross entropy loss`

而為什麼是`stochastic`是因為，我們每一次都是`隨機`拿一個sample來計算loss，每一次的loss導向的結果可能差異較大

所以較好的方式是一次看很多個sample(ex. 100)，也就是`mini-batch Gradient Descent`，這樣計算出來的loss會讓我們導向這100個sample中整體趨向的結果

而`learning rate`的取值，通常一開始可以取得較大，讓收斂速度快一點，到越後面要越來越小

`learning rate`的值越大，收斂越快，但也可能導致發散而得到錯誤結果，反之，收斂慢但較準確

---
## Neural Language Models
---

在這一小節，我們想用前面學到的Neural Network來建構一個`Language models(LM)`

給定前三個字，我們要去預估接下來的字(upcoming word)

我們可以利用已經訓練好(pretraining)的word embeddings當作input，而要預估的upcoming word的embedding當作label

但我們可以把word用one-hot vector表示，多加一層`projection layer`

$$e=E(Ex_{1}, x_{2}, ..., Ex)$$

這樣分離出來的好處是，我們這麼以來，也能同時訓練到`E`

把這種後一個字與前三個字的關係，也訓練到Embedding裡面去






