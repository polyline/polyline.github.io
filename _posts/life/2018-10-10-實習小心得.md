---
layout: article_post
title:  "實習小心得"
categories: life nlp
excerpt_separator: <!--more-->
---

從上個月開始，我開始到台北的一間新創公司實習

這家公司主要是做`NLP`(自然語言處理)及`NLG`(自然語言生成)，剛好和我在日本研究室相關

當時在日本也算是懞懞懂懂才申請到自然語言相關的實驗室

而在這一年中，其實也因為本來沒有實力基礎，沒有做出什麼厲害的成果來

但是，卻也幫我對這領域打下良好基礎，很多東西我都大概聽過、理解它的理論

我缺乏的是要將其好好系統化整理、紀錄，然後加強實作的部分

現在在公司，我有一位`NLP Team`的主管會帶我

我主要在讀情緒文本分類(Sentiment Classification)相關的論文(Paper Review)，並想辦法實作出來

在剛剛實習這兩週，我已經試過`Bayes`、`SVM`、`TextCNN`來實作

深深覺得程式碼的能力要更加強，尤其是對`gensim`、`pytorch`、`tensorflow`、`sk-learn`這幾個套件的熟悉程度還不夠

我會繼續用這樣邊寫邊學的方式來學習，並且要整理程筆記記錄下來

*2018/10/10*

---

實習過了快一個月了，每個禮拜兩天的實習已經成為我最充實的時間

雖然只有一個簡單的任務，就是做情緒分析

但是我要用什麼方法、怎麼讓效果更好，讓我要閱讀大量文章跟論文來做

像是，當時候發現了一些問題

1. 直接使用`word embedding`會導致無法判斷反義詞
2. 分詞的問題，像是如果`不開心`，如果分詞分成`不` `開心`就會失敗，因為`不`屬於停用詞，沒有對應的詞向量
3. 改用了`char-level CNN`，但是太多無關（無情緒）的字會洗掉情緒字
4. 不能使用`data augmentation`，跟第一點同原因

所以我發現，與其說哪一個model能做的比較好，不如說怎樣組裝各種方法能達到比較好效果

因此，最後表現的比較好的有以下幾種

1. TextCNN + Standard Tokenizer + pre-trained WB
2. TextCNN + char-level + one-hot WB
3. similar words

除此之外，我也學到了怎麼更有組織的寫code，讓別人能夠一目暸然  

還有接觸到很多工具，AWS, Robot3T, Celebro等

*2018/11/4*
