---
layout: article_post
title:  "HMM"
categories: cs nlp
excerpt_separator: <!--more-->
---

---
## 介紹
---

最近研究跟讀書會，剛好同時接觸到`Markov Chain`這個名詞

往下鑽研才會發現，這些概念都是相互綁在一起的`Probability`, `Graphic Model`, `Markov Chain`, `HMM`等

讓我們在這篇文章中，把相關的知識都整理一下吧！

*PS. 在我的文章中，會盡量將第一次出現的專有名詞同時中英文標註，我自己在查資料時常常會因為翻譯的問題而看不懂*

<!--more-->

---
## HMM
---

一個`HMM Model`可以歸類成三個問題

1. Likelihood
2. Decoding
3. Learning

首先，讓我們回顧一下什麼是`Markov Assumption`

$$P(q_{t} \mid q_{t-1}, q_{t-2}, ...) = P(q_{t} \mid q_{t-1})$$

也就是假設我們現在的狀態$q_{t}$只依賴於前一個狀態$q_{t-1}$，$P(q_{t} \mid q_{t-1})$就是`轉移機率(transition probability)`

在Hidden Markov Model中，除了狀態轉移的`transition probability`，還有從狀態到觀測值的`發射機率(emission probability)`

$$P(y_{t} \mid q_{t}), y_{t}為在t時的觀測值$$

所以在`HMM`中，我們用矩陣來表示這兩個機率

1. 轉移矩陣(transition matrix)$ A_{kxk}$
2. 發射矩陣(emission matrix)$ B_{kxL}$

我們用三個參數來表示一個`HMM Model`，$A代表的是狀態之間的轉移矩陣，B代表的是狀態到觀測值的發射矩陣$，而$\pi$是我們的起始狀態

$$\Lambda = \{ A, B, \pi \}$$

假設我們現在有三個觀測值，我們想求這樣的觀測值出現的機率為多少

$$P(y_{1}, y_{2}, y_{3}) = \sum_{q_{1}=1}^{k} \sum_{q_{2}=1}^{k} \sum_{q_{3}=1}^{k} P(y_{1}, y_{2}, y_{3}, q_{1}, q_{2}, q_{3}) \\ = \sum_{q_{1}=1}^{k} \sum_{q_{2}=1}^{k} \sum_{q_{3}=1}^{k} P(y_{3} \mid q_{3}) P(q_{3} \mid q_{2}) P(y_{2} \mid q_{2}) P(q_{2} \mid q_{1}) P(y_{1} \mid q_{1}) P(q_{1})$$

在上面的式子中，我們第一步先將狀態$q$加進去，之後利用`Bayes' rules`的條件機率定義展開，並且藉由`Markov Assumption`將前面狀態及前面觀測值消除(狀態只跟前一個狀態有關，觀測也只跟現在這個狀態有關)

但是，這樣的結果還是非常難計算，我們需要做那麼多次的sum，假設我們現在觀測值不只三個的話，我們有$T$個觀測值

那我們總共需要做$T$的$sum$，每次做$k$次，所以總共有$k^{T}$的計算量，而且在現實應用中，$k$和$T$都是很大的值

因此，我們可以利用程式設計中的`DP`觀念來解決這個問題，我們稱之為`Forward Algorithm, Backward Algorithm`

首先，先定義兩個變數$\alpha , \beta$

$$\alpha_{i}(t) = P(y_{1}, ..., y_{t}, q_{t} = i \mid \lambda)$$

$$\beta_{i}(t) = P(y_{t+1}, ..., y_{T}, q_{t} = i \mid \lambda)$$

稍微解釋一下，$\alpha$即`狀態t`跟`觀測值1到t`的`joint probability`，$\beta$則是`狀態t`跟`觀測值t+1到T`的`joint probability`

我們接下來從$\alpha_{i}(1)$開始推導，看能不能找出什麼規律

$$\alpha_{i}(1) = P(y_{1}, q_{1} = i \mid \lambda) = P(y_{1} \mid q_{1} = i) \times P(q_{1})$$

$$\alpha_{j}(2) = P(y_{1}, y_{2}, q_{2} = j \mid \lambda) = \sum_{i=1}^{k} P(y_{1}, y_{2}, q_{1}=i, q_{2}=j) = 

P(y_{2} \mid q_{2} = j) \sum_{i=1}^{k} P(q_{2}=j \mid q_{1}=i) \times \alpha_{i}(1)$$

從此我們可以發現，可以找出規律寫出以下式子

$$\alpha_{i}(T) = P(y_{1}, .., y_{T}, q_{T} = i) = P(y_{T} \mid q_{T} = i) \sum_{i=1}^{k} P(q_{T}=j \mid q_{T-1} = i) \times \alpha_{i}(T-1) = b_{i}(T) \sum_{i=1}^{k} a_{i, j} \alpha_{i}(T-1)$$

經過這樣的式子，我們可以將計算量$k^{T}$降到$kT$，因為每一個$\alpha$都只需做計算量k次的$sum$，也就是`forward formula`

而還記得我們原來要求的是$P(y_{1}, ..., y_{T})$而我們現在有的是$\alpha_{i}(T) = P(y_{1}, ..., y_{T}, q_{T} = i)$

所以我們只要把$q_{T}$積分起來，就能得到我們要的式子了

$$P(y_{1}, ..., y_{T}) = \sum_{i=1}^{k}\alpha_{i}(T)$$

---
## Reference
---

1. [Youtube上講解HMM影片，UBC大學的影片](https://www.youtube.com/watch?v=jY2E6ExLxaw)
2. [LDA數學八卦，關於馬可夫鏈的章節]
3. [Graphic Model]
4. [徐亦達機器學習課程](https://www.youtube.com/watch?v=Ji6KbkyNmk8)
	-非常清楚的講解HMM的推導



